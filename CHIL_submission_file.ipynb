{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed977fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn import preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f898d2",
   "metadata": {},
   "source": [
    "### Merge full mimiic data with mimic data predictions for smoking status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c279c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Retrieving merged_data -- can't show due to MIMIC Privacy Policy\n",
    "full_data_df = pd.read_csv(\"full_data_df_no_index.csv\")\n",
    "pred_mimic_df = pd.read_csv(\"...\") # Should be the csv file with mimic smoking status predictions for each entry\n",
    "pred_mimic_df = pred_mimic_df.rename(columns={'SUBJECT_ID': 'subject_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904fc33a",
   "metadata": {},
   "source": [
    "#### full_data_df contains 6361 rows and 130 columns (including subject_id, age, echo, etc...)\n",
    "#### pred_mimic_df contains 34312 rows and 46 columns (including subject_id, SMOKING_STATUS, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b34a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(full_data_df, pred_mimic_df[[\"subject_id\",\"SMOKING_STATUS\"]], on=[\"subject_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ce87fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2058\n",
       "3    1413\n",
       "4    1171\n",
       "2      93\n",
       "0      64\n",
       "Name: SMOKING_STATUS, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"SMOKING_STATUS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a900c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2058\n",
       "3    1413\n",
       "4    1171\n",
       "2      93\n",
       "Name: SMOKING_STATUS, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Droppping 0 labels to ensure we only have 4 possible smoking status labels\n",
    "# 0 labels derived from merging two dataframes where some entries may not have a prediction\n",
    "merged_df = merged_df.drop(merged_df[merged_df[\"SMOKING_STATUS\"] == 0].index)\n",
    "merged_df[\"SMOKING_STATUS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f986148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting integers to weekdays\n",
    "def int_to_weekday(row):\n",
    "    r = int(row)\n",
    "    if r == 0:\n",
    "        return 'sunday'\n",
    "    elif r == 1:\n",
    "        return \"monday\"\n",
    "    elif r == 2:\n",
    "        return \"tuesday\"\n",
    "    elif r == 3:\n",
    "        return \"wednesday\"\n",
    "    elif r == 4:\n",
    "        return \"thursday\"\n",
    "    elif r== 5:\n",
    "        return \"friday\"\n",
    "    else:\n",
    "        return \"saturday\"\n",
    "\n",
    "merged_df[\"icu_adm_weekday\"] = merged_df[\"icu_adm_weekday\"].apply(int_to_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22ae029",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"first_careunit\"] = merged_df[\"first_careunit\"].astype('category')\n",
    "merged_df[\"first_careunit\"] = merged_df[\"first_careunit\"].cat.reorder_categories([\"SICU\", \"MICU\"])\n",
    "\n",
    "merged_df[\"gender\"] = merged_df[\"gender\"].astype(\"category\")\n",
    "merged_df[\"gender\"] = merged_df[\"gender\"].cat.reorder_categories([\"M\", \"F\"])\n",
    "\n",
    "merged_df[\"icu_adm_weekday\"] = merged_df[\"icu_adm_weekday\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b9678",
   "metadata": {},
   "source": [
    "### Viewing the finalized merged dataframe\n",
    "\n",
    "#### 4735 rows and 131 columns (including subject_id, echo, mort_28_day, SMOKING_STATUS, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df # Unable to show due to MIMIC Privacy Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5719af",
   "metadata": {},
   "source": [
    "### Defining helper functions to calculate causal effects w.r.t effect restoration from measurement bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b14608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_models(dataframe):\n",
    "    '''\n",
    "    Given a pre-processed MIMIC + proxy prediction dataframe, train four logistic regression models using smf.logit. \n",
    "    The formula strings will be hard-coded into the function. The assumptions for these models are:\n",
    "        1) Categorical smoking categories \n",
    "        2) Not all feature are binary, but at least the output (mort_28_day) and treatment (echo) should be binary\n",
    "    '''\n",
    "    \n",
    "    # Calculating P(y | u*, a, c) --> y ~ u* + a + c\n",
    "    fstring = 'mort_28_day ~ echo + first_careunit + age + gender + weight + saps + sofa + elix_score + vent + \\\n",
    "            vaso + icu_adm_weekday + icu_adm_hour + icd_chf + icd_afib + icd_renal + icd_liver + icd_copd + \\\n",
    "            icd_cad + icd_stroke + icd_malignancy + vs_heart_rate_first + vs_map_first + vs_temp_first + \\\n",
    "            lab_hemoglobin_first + lab_platelet_first + lab_wbc_first + lab_ph_first + lab_chloride_first + \\\n",
    "            lab_sodium_first + lab_bun_first + lab_bicarbonate_first + lab_pco2_first + lab_creatinine_first + \\\n",
    "            lab_potassium_first + lab_po2_first + lab_lactate_first + sedative + vs_cvp_flag + \\\n",
    "            lab_creatinine_kinase_flag + lab_bnp_flag + lab_troponin_flag + SMOKING_STATUS'\n",
    "    eq1 = smf.logit(fstring, data=dataframe)\n",
    "    eq1_model = eq1.fit(disp=0)\n",
    "    \n",
    "    # Calculating P(u* | a, c)\n",
    "    f_string2 = \"SMOKING_STATUS ~ echo + first_careunit + age + gender + weight + saps + sofa + elix_score + vent + \\\n",
    "            vaso + icu_adm_weekday + icu_adm_hour + icd_chf + icd_afib + icd_renal + icd_liver + icd_copd + \\\n",
    "            icd_cad + icd_stroke + icd_malignancy + vs_heart_rate_first + vs_map_first + vs_temp_first + \\\n",
    "            lab_hemoglobin_first + lab_platelet_first + lab_wbc_first + lab_ph_first + lab_chloride_first + \\\n",
    "            lab_sodium_first + lab_bun_first + lab_bicarbonate_first + lab_pco2_first + lab_creatinine_first + \\\n",
    "            lab_potassium_first + lab_po2_first + lab_lactate_first + sedative + vs_cvp_flag + \\\n",
    "            lab_creatinine_kinase_flag + lab_bnp_flag + lab_troponin_flag\"\n",
    "    eq2 = smf.mnlogit(f_string2, data=dataframe)\n",
    "    eq2_model = eq2.fit(disp=0)\n",
    "    \n",
    "    # Calculating P(c,u*) --> approximates to (P(u* | c) - eu) / (1- eu - &u) * 1 / N\n",
    "    f_string3 = \"SMOKING_STATUS ~ first_careunit + age + gender + weight + saps + sofa + elix_score + vent + \\\n",
    "            vaso + icu_adm_weekday + icu_adm_hour + icd_chf + icd_afib + icd_renal + icd_liver + icd_copd + \\\n",
    "            icd_cad + icd_stroke + icd_malignancy + vs_heart_rate_first + vs_map_first + vs_temp_first + \\\n",
    "            lab_hemoglobin_first + lab_platelet_first + lab_wbc_first + lab_ph_first + lab_chloride_first + \\\n",
    "            lab_sodium_first + lab_bun_first + lab_bicarbonate_first + lab_pco2_first + lab_creatinine_first + \\\n",
    "            lab_potassium_first + lab_po2_first + lab_lactate_first + sedative + vs_cvp_flag + \\\n",
    "            lab_creatinine_kinase_flag + lab_bnp_flag + lab_troponin_flag\"\n",
    "    eq3 = smf.glm(f_string3, data=dataframe)\n",
    "    eq3_model = eq3.fit(disp=0)\n",
    "    \n",
    "    # Calculating P(y | a, c)\n",
    "    f_string4 = 'mort_28_day ~ echo + first_careunit + age + gender + weight + saps + sofa + elix_score + vent + \\\n",
    "                vaso + icu_adm_weekday + icu_adm_hour + icd_chf + icd_afib + icd_renal + icd_liver + icd_copd + \\\n",
    "                icd_cad + icd_stroke + icd_malignancy + vs_heart_rate_first + vs_map_first + vs_temp_first + \\\n",
    "                lab_hemoglobin_first + lab_platelet_first + lab_wbc_first + lab_ph_first + lab_chloride_first + \\\n",
    "                lab_sodium_first + lab_bun_first + lab_bicarbonate_first + lab_pco2_first + lab_creatinine_first + \\\n",
    "                lab_potassium_first + lab_po2_first + lab_lactate_first + sedative + vs_cvp_flag + \\\n",
    "                lab_creatinine_kinase_flag + lab_bnp_flag + lab_troponin_flag'\n",
    "    eq4 = smf.logit(f_string4, data=dataframe)\n",
    "    eq4_model = eq4.fit(disp=0)\n",
    "    \n",
    "    # Calculating P(a|c)\n",
    "    f_string5 = \"echo ~ first_careunit + age + gender + weight + saps + sofa + elix_score + vent + \\\n",
    "                vaso + icu_adm_weekday + icu_adm_hour + icd_chf + icd_afib + icd_renal + icd_liver + icd_copd + \\\n",
    "                icd_cad + icd_stroke + icd_malignancy + vs_heart_rate_first + vs_map_first + vs_temp_first + \\\n",
    "                lab_hemoglobin_first + lab_platelet_first + lab_wbc_first + lab_ph_first + lab_chloride_first + \\\n",
    "                lab_sodium_first + lab_bun_first + lab_bicarbonate_first + lab_pco2_first + lab_creatinine_first + \\\n",
    "                lab_potassium_first + lab_po2_first + lab_lactate_first + sedative + vs_cvp_flag + \\\n",
    "                lab_creatinine_kinase_flag + lab_bnp_flag + lab_troponin_flag\"\n",
    "    eq5 = smf.logit(f_string5, data=dataframe)\n",
    "    eq5_model = eq5.fit(disp=0)\n",
    "    \n",
    "    return eq1_model, eq2_model, eq3_model, eq4_model, eq5_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac2713",
   "metadata": {},
   "source": [
    "### Implementing Risk Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3db7275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_ratio(dataframe, model1, model2, model3, model4, model5):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as five trained models \n",
    "    from generate_models(), calculate the risk ratio as defined by: \n",
    "    causal_effect = summation(c,u){ p(c,u) * ( E[Y=1 | A=1,c,u*] / E[Y=1 | A=0,c,u*] ) }\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(c,u*) --> approximates to (P(u* | c) - eu) / (1- eu - &u) * 1 / N --> P(u | c)\n",
    "            d) model4 = P(y | a, c)\n",
    "            e) model5 = P(a | c)\n",
    "        4) Default prediction is probability of getting 1 due to how statsmodels works\n",
    "    '''\n",
    "    \n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    # Understanding Matrix of Error Adjustments\n",
    "    confusion = [\n",
    "                    [8, 0, 2, 1],\n",
    "                    [4, 4, 3, 0],\n",
    "                    [1, 0, 14, 1],\n",
    "                    [1, 0, 1, 61]\n",
    "                ] # rows represent the ground truth labels and cols represents the predicted labels\n",
    "\n",
    "    error_mat = [\n",
    "                    [8/11, 0, 2/11, 1/11],\n",
    "                    [4/11, 4/11, 3/11, 0],\n",
    "                    [1/16, 0, 14/16, 1/16],\n",
    "                    [1/63, 0, 1/63, 61/63]\n",
    "                ] # rows represent U* and cols represent U\n",
    "    inverse = np.linalg.inv(error_mat)\n",
    "    \n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model5.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model5.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "        \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * inverse[2][0] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * inverse[2][0] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * inverse[2][0] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * inverse[2][0] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    comp_0 = upper_0a / lower_0b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * inverse[2][1] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * inverse[2][1] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * inverse[2][1] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * inverse[2][1] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    comp_1 = upper_1a / lower_1b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * inverse[2][2] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * inverse[2][2] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * inverse[2][2] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * inverse[2][2] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    comp_2 = upper_2a / lower_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * inverse[2][3] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * inverse[2][3] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * inverse[2][3] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * inverse[2][3] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    comp_3 = upper_3a / lower_3b\n",
    "    \n",
    "    # Getting P(u | c) = summation{u}{P(A=0, c, y=0, u) + P(A=1, c, y=0, u) + P(A=0, c, y=1, u) + P(A=0, c, y=1, u)}\n",
    "    \n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "    print([np.mean(comp_0 * prob_u0_c), np.mean(comp_1 * prob_u1_c), \n",
    "           np.mean(comp_2 * prob_u2_c), np.mean(comp_3 * prob_u3_c)])\n",
    "    rr = np.mean(comp_0 * prob_u0_c) + np.mean(comp_1 * prob_u1_c) + np.mean(comp_2 * prob_u2_c) \n",
    "            + np.mean(comp_3 * prob_u3_c)\n",
    "    return rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae5d0f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4838000219913868, 0.054784920499759755, 0.16731889416457849, 0.18447476501081997]\n",
      "0.8903786016665449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8903786016665449"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2, m3, m4, m5 = generate_models(merged_df)\n",
    "risk_ratio(merged_df, m1, m2, m3, m4, m5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18fa90",
   "metadata": {},
   "source": [
    "### Bootstrapping Error Rate Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c87a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_ratio_bootstrap(dataframe, model1, model2, model3, model4, model5, error_mat):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as five trained models \n",
    "    from generate_models(), calculate the risk ratio as defined by: \n",
    "    causal_effect = summation(c,u){ p(c,u) * ( E[Y=1 | A=1,c,u*] / E[Y=1 | A=0,c,u*] ) }\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(c,u*) --> approximates to (P(u* | c) - eu) / (1- eu - &u) * 1 / N --> P(u | c)\n",
    "            d) model4 = P(y | a, c)\n",
    "            e) model5 = P(a | c)\n",
    "        4) Default prediction is probability of getting 1 due to how statsmodels works\n",
    "    '''\n",
    "    \n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    # Inversing Error Rate Matrices\n",
    "    inverse = np.linalg.inv(error_mat)\n",
    "    \n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model5.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model5.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "        \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * inverse[2][0] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * inverse[2][0] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * inverse[2][0] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * inverse[2][0] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    comp_0 = upper_0a / lower_0b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * inverse[2][1] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * inverse[2][1] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * inverse[2][1] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * inverse[2][1] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    comp_1 = upper_1a / lower_1b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * inverse[2][2] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * inverse[2][2] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * inverse[2][2] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * inverse[2][2] + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    comp_2 = upper_2a / lower_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * inverse[2][3] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * inverse[2][3] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * inverse[2][3] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * inverse[2][3] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    comp_3 = upper_3a / lower_3b\n",
    "    \n",
    "    # Getting P(u | c) = summation{u}{P(A=0, c, y=0, u) + P(A=1, c, y=0, u) + P(A=0, c, y=1, u) + P(A=0, c, y=1, u)}\n",
    "    \n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "    rr = np.mean(comp_0 * prob_u0_c) + np.mean(comp_1 * prob_u1_c) + np.mean(comp_2 * prob_u2_c) \n",
    "        + np.mean(comp_3 * prob_u3_c)\n",
    "    return rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c53e94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(dataframe, model1, model2, model3, model4, model5):\n",
    "    '''\n",
    "    Given a dataframe and 5 models generated from generate_models(), bootstrap the testing set for n2c2 2006 smoking\n",
    "    dataset to get different error rate matrices to test robustness of the risk ratio casual effect.\n",
    "    Utilize predict_bootstrap_2006.py to generate pickle files that store the confusion matrices.\n",
    "    '''\n",
    "    \n",
    "    # Iterating through the bootstrapped confusion matrices \n",
    "    # \"iterations\" var depends on how many bootstrapped confusion matrics were generated\n",
    "    # Default in predict_bootstrap_2006.py is 10\n",
    "    iterations = 10 \n",
    "    rr_arr = []\n",
    "    for x in range(iterations):\n",
    "        # Access each pickle file containing the confusion matrix\n",
    "        f = open(\"...\", \"rb\")  # First input should be the bootstrapped matrices (pkl file)\n",
    "        con_matrix = pickle.load(f)\n",
    "        res = con_matrix/con_matrix.sum(axis=1)[:,None]\n",
    "        rr = risk_ratio_bootstrap(dataframe, model1, model2, model3, model4, model5, res)\n",
    "        rr_arr.append(rr)\n",
    "    \n",
    "    print(rr_arr)\n",
    "    return sum(rr_arr) / len(rr_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ad238fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8909875802243661, 0.8904475988807712, 0.890596509707885, 0.8906791999431076, 0.8900207140785991, 0.8907383965807781, 0.8895413941042646, 0.8908275785985995, 0.8954674682229409, 0.8904210990774163]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8909727539418728"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2, m3, m4, m5 = generate_models(merged_df)\n",
    "bootstrap(merged_df, m1, m2, m3, m4, m5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6cbf51",
   "metadata": {},
   "source": [
    "### Implementing OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8025cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_ratio(dataframe, model1, model2, model3, model4, model5):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as five trained models \n",
    "    from generate_models(), calculate the odds ratio as defined by: \n",
    "    causal_effect = (P(Y^{a=1}=1) * P(Y^{a=0}=0)) / (P(Y^{a=1}=0) * P(Y^{a=0}=1))\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(c,u*) --> approximates to (P(u* | c) - eu) / (1- eu - &u) * 1 / N --> P(u | c)\n",
    "            d) model4 = P(y | a, c)\n",
    "            e) model5 = P(a | c)\n",
    "        4) Default prediction is probability of getting 1 due to how statsmodels works\n",
    "    '''\n",
    "    \n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    # Creating Matrix of Error Adjustments\n",
    "    confusion = [\n",
    "                    [8, 0, 2, 1],\n",
    "                    [4, 4, 3, 0],\n",
    "                    [1, 0, 14, 1],\n",
    "                    [1, 0, 1, 61]\n",
    "                ] # rows represent the ground truth labels and cols represents the predicted labels\n",
    "\n",
    "    error_mat = [\n",
    "                    [8/11, 0, 2/11, 1/11],\n",
    "                    [4/11, 4/11, 3/11, 0],\n",
    "                    [1/16, 0, 14/16, 1/16],\n",
    "                    [1/63, 0, 1/63, 61/63]\n",
    "                ] # rows represent U* and cols represent U\n",
    "    inverse = np.linalg.inv(error_mat)\n",
    "    \n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model5.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model5.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * inverse[2][0] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * inverse[2][0] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * inverse[2][0] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * inverse[2][0] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * inverse[2][1] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * inverse[2][1] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * inverse[2][1] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * inverse[2][1] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * inverse[2][2] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * inverse[2][2] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * inverse[2][2] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * inverse[2][2] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * inverse[2][3] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * inverse[2][3] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * inverse[2][3] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * inverse[2][3] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    # Getting P(u | c) = summation{u}{P(A=0, c, y=0, u) + P(A=1, c, y=0, u) + P(A=0, c, y=1, u) + P(A=0, c, y=1, u)}\n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "    numerator_a = np.sum(upper_0a * prob_u0_c) + np.sum(upper_1a * prob_u1_c) + np.sum(upper_2a * prob_u2_c) \n",
    "                  + np.sum(upper_3a * prob_u3_c)\n",
    "    numerator_b = np.sum((1 - lower_0b) * prob_u0_c) + np.sum((1 - lower_1b) * prob_u1_c) \n",
    "                  + np.sum((1 - lower_2b) * prob_u2_c) + np.sum((1 - lower_3b) * prob_u3_c)\n",
    "     \n",
    "    denominator_a = np.sum((1 - upper_0a) * prob_u0_c) + np.sum((1 - upper_1a) * prob_u1_c) \n",
    "                    + np.sum((1 - upper_2a) * prob_u2_c) + np.sum((1 - upper_3a) * prob_u3_c)\n",
    "    denominator_b = np.sum(lower_0b * prob_u0_c) + np.sum(lower_1b * prob_u1_c) + np.sum(lower_2b * prob_u2_c) \n",
    "                    + np.sum(lower_3b * prob_u3_c)\n",
    "    \n",
    "    numerator = numerator_a * numerator_b\n",
    "    denominator = denominator_a * denominator_b\n",
    "    \n",
    "    return numerator / denominator\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca656b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8901509440891107"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2, m3, m4, m5 = generate_models(merged_df)\n",
    "odds_ratio(merged_df, m1, m2, m3, m4, m5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56844e59",
   "metadata": {},
   "source": [
    "### Bootstrapping for OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dafa3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_ratio_bootstrap(dataframe, model1, model2, model3, model4, model5, error_mat):\n",
    "     '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as five trained models \n",
    "    from generate_models(), calculate the odds ratio as defined by: \n",
    "    causal_effect = (P(Y^{a=1}=1) * P(Y^{a=0}=0)) / (P(Y^{a=1}=0) * P(Y^{a=0}=1))\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(c,u*) --> approximates to (P(u* | c) - eu) / (1- eu - &u) * 1 / N --> P(u | c)\n",
    "            d) model4 = P(y | a, c)\n",
    "            e) model5 = P(a | c)\n",
    "        4) Default prediction is probability of getting 1 due to how statsmodels works\n",
    "    '''\n",
    "    \n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    inverse = np.linalg.inv(error_mat)\n",
    "    \n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model5.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model5.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * inverse[2][0] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * inverse[2][0] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * inverse[2][0] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * inverse[2][0] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * inverse[2][1] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * inverse[2][1] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * inverse[2][1] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * inverse[2][1] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * inverse[2][2] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * inverse[2][2] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * inverse[2][2] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * inverse[2][2] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * inverse[2][3] \n",
    "            + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * inverse[2][3] \n",
    "            + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * inverse[2][3] \n",
    "            + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * inverse[2][3] \n",
    "            + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    # Getting P(u | c) = summation{u}{P(A=0, c, y=0, u) + P(A=1, c, y=0, u) + P(A=0, c, y=1, u) + P(A=0, c, y=1, u)}\n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "    numerator_a = np.sum(upper_0a * prob_u0_c) + np.sum(upper_1a * prob_u1_c) + np.sum(upper_2a * prob_u2_c) \n",
    "                  + np.sum(upper_3a * prob_u3_c)\n",
    "    numerator_b = np.sum((1 - lower_0b) * prob_u0_c) + np.sum((1 - lower_1b) * prob_u1_c) \n",
    "                  + np.sum((1 - lower_2b) * prob_u2_c) + np.sum((1 - lower_3b) * prob_u3_c)\n",
    "     \n",
    "    denominator_a = np.sum((1 - upper_0a) * prob_u0_c) + np.sum((1 - upper_1a) * prob_u1_c) \n",
    "                    + np.sum((1 - upper_2a) * prob_u2_c) + np.sum((1 - upper_3a) * prob_u3_c)\n",
    "    denominator_b = np.sum(lower_0b * prob_u0_c) + np.sum(lower_1b * prob_u1_c) + np.sum(lower_2b * prob_u2_c) \n",
    "                    + np.sum(lower_3b * prob_u3_c)\n",
    "    \n",
    "    numerator = numerator_a * numerator_b\n",
    "    denominator = denominator_a * denominator_b\n",
    "    \n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05c1a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_or(dataframe, model1, model2, model3, model4, model5):\n",
    "    '''\n",
    "    Given a dataframe and 5 models generated from generate_models(), bootstrap the testing set for n2c2 2006 smoking\n",
    "    dataset to get different error rate matrices to test robustness of the odds ratio casual effect.\n",
    "    Utilize predict_bootstrap_2006.py to generate pickle files that store the confusion matrices.\n",
    "    '''\n",
    "    \n",
    "    # Iterating through the bootstrapped confusion matrices \n",
    "    # \"iterations\" var depends on how many bootstrapped confusion matrics were generated\n",
    "    # Default in predict_bootstrap_2006.py is 10\n",
    "    iterations = 10 \n",
    "    o_r_arr = []\n",
    "    for x in range(iterations):\n",
    "        # Access each pickle file containing the confusion matrix\n",
    "        f = open(\"...\", \"rb\")  # First input should be the bootstrapped matrices (pkl file)\n",
    "        con_matrix = pickle.load(f)\n",
    "        res = con_matrix/con_matrix.sum(axis=1)[:,None]\n",
    "        o_r = odds_ratio_bootstrap(dataframe, model1, model2, model3, model4, model5, res)\n",
    "        o_r_arr.append(o_r)\n",
    "    \n",
    "    print(o_r_arr)\n",
    "    return sum(o_r_arr) / len(o_r_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e87266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8899959374745883, 0.8898435307077874, 0.8896619266716497, 0.8897183772991344, 0.8895934058255944, 0.8899128538729955, 0.888273217775345, 0.8900140205941309, 0.9027932942358518, 0.8890330213136267]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8908839585770704"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2, m3, m4, m5 = generate_models(merged_df)\n",
    "bootstrap_or(merged_df, m1, m2, m3, m4, m5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d8ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
