{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed977fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning, HessianInversionWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b38e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out warnings in case they appear to prevent flooding of outputs\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "warnings.simplefilter('ignore', HessianInversionWarning)\n",
    "warnings.simplefilter('ignore', pd.errors.DtypeWarning)\n",
    "warnings.simplefilter('ignore', RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a35705b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ci_95(arr):\n",
    "    \"\"\"\n",
    "    Helper function to calculate 95% interval given an array\n",
    "    \"\"\"\n",
    "    return [np.percentile(arr, 2.5), np.percentile(arr, 97.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f898d2",
   "metadata": {},
   "source": [
    "### Merge full mimiic data with mimic data predictions for smoking status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c279c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving merged_data -- can't show due to MIMIC Privacy Policy\n",
    "full_data_df = pd.read_csv(\"full_data_df_no_index.csv\")\n",
    "pred_mimic_df = pd.read_csv(\"...\")# Should be the csv file with mimic smoking status predictions for each entry\n",
    "pred_mimic_df = pred_mimic_df.rename(columns={'SUBJECT_ID': 'subject_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904fc33a",
   "metadata": {},
   "source": [
    "#### full_data_df contains 6361 rows and 130 columns (including subject_id, age, echo, etc...)\n",
    "#### pred_mimic_df contains 34312 rows and 46 columns (including subject_id, SMOKING_STATUS, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b34a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(full_data_df, pred_mimic_df[[\"subject_id\",\"SMOKING_STATUS\"]], on=[\"subject_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ce87fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2058\n",
       "3    1413\n",
       "4    1171\n",
       "2      93\n",
       "0      64\n",
       "Name: SMOKING_STATUS, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"SMOKING_STATUS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a900c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2058\n",
       "3    1413\n",
       "4    1171\n",
       "2      93\n",
       "Name: SMOKING_STATUS, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Droppping 0 labels to ensure we only have 4 possible smoking status labels\n",
    "# 0 labels derived from merging two dataframes where some entries may not have a prediction\n",
    "merged_df = merged_df.drop(merged_df[merged_df[\"SMOKING_STATUS\"] == 0].index)\n",
    "merged_df[\"SMOKING_STATUS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f986148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting integers to weekdays\n",
    "def int_to_weekday(row):\n",
    "    r = int(row)\n",
    "    if r == 0:\n",
    "        return 'sunday'\n",
    "    elif r == 1:\n",
    "        return \"monday\"\n",
    "    elif r == 2:\n",
    "        return \"tuesday\"\n",
    "    elif r == 3:\n",
    "        return \"wednesday\"\n",
    "    elif r == 4:\n",
    "        return \"thursday\"\n",
    "    elif r== 5:\n",
    "        return \"friday\"\n",
    "    else:\n",
    "        return \"saturday\"\n",
    "\n",
    "merged_df[\"icu_adm_weekday\"] = merged_df[\"icu_adm_weekday\"].apply(int_to_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d22ae029",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"first_careunit\"] = merged_df[\"first_careunit\"].astype('category')\n",
    "merged_df[\"first_careunit\"] = merged_df[\"first_careunit\"].cat.reorder_categories([\"SICU\", \"MICU\"])\n",
    "\n",
    "merged_df[\"gender\"] = merged_df[\"gender\"].astype(\"category\")\n",
    "merged_df[\"gender\"] = merged_df[\"gender\"].cat.reorder_categories([\"M\", \"F\"])\n",
    "\n",
    "merged_df[\"icu_adm_weekday\"] = merged_df[\"icu_adm_weekday\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b9678",
   "metadata": {},
   "source": [
    "### Viewing the finalized merged dataframe\n",
    "\n",
    "#### 4735 rows and 131 columns (including subject_id, echo, mort_28_day, SMOKING_STATUS, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df # Unable to show due to MIMIC Privacy Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5719af",
   "metadata": {},
   "source": [
    "### Defining helper functions to calculate causal effects w.r.t effect restoration from measurement bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8b14608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_models(dataframe):\n",
    "    '''\n",
    "    Given a pre-processed MIMIC + proxy prediction dataframe, train three logistic regression models \n",
    "    using smf.logit. \n",
    "    The formula strings will be hard-coded into the function. The assumptions for these models are:\n",
    "        1) Categorical smoking categories \n",
    "        2) Not all feature are binary, but at least the output (mort_28_day) and treatment (echo) \n",
    "           should be binary\n",
    "    '''\n",
    "    \n",
    "    # Calculating P(y | u*, a, c) --> y ~ u* + a + c for each u* label in [1,2,3,4] \n",
    "    fstring = 'mort_28_day ~ echo + age + weight + saps + sofa + elix_score + vent + \\\n",
    "            vaso + icu_adm_hour + icd_chf + icd_afib + icd_renal + icd_liver + icd_copd + \\\n",
    "            icd_cad + icd_stroke + icd_malignancy + vs_heart_rate_first + vs_map_first + vs_temp_first + \\\n",
    "            lab_hemoglobin_first + lab_platelet_first + lab_wbc_first + lab_chloride_first + \\\n",
    "            lab_sodium_first + lab_bun_first + lab_bicarbonate_first + lab_creatinine_first + \\\n",
    "            lab_potassium_first + vs_cvp_flag + lab_creatinine_kinase_flag + lab_bnp_flag + gender + \\\n",
    "            lab_troponin_flag + first_careunit + icu_adm_weekday + lab_ph_first + lab_pco2_first + \\\n",
    "            lab_po2_first + lab_lactate_first + sedative + C(SMOKING_STATUS)'\n",
    "    eq1 = smf.logit(fstring, data=dataframe)\n",
    "    eq1_model = eq1.fit(disp=0)\n",
    "    \n",
    "    # Calculating P(u* | a, c)\n",
    "    f_string2 = \"SMOKING_STATUS ~ echo + first_careunit + age + gender + weight + saps + sofa + elix_score + \\\n",
    "            vent + vaso + icu_adm_weekday + icu_adm_hour + icd_chf + icd_afib + icd_renal + icd_liver + \\\n",
    "            icd_copd + icd_cad + icd_stroke + icd_malignancy + vs_heart_rate_first + vs_map_first + \\\n",
    "            vs_temp_first + lab_hemoglobin_first + lab_platelet_first + lab_wbc_first + lab_ph_first + \\\n",
    "            lab_sodium_first + lab_bun_first + lab_bicarbonate_first + lab_pco2_first + lab_creatinine_first + \\\n",
    "            lab_chloride_first + lab_potassium_first + lab_po2_first + lab_lactate_first + sedative + \\\n",
    "            vs_cvp_flag + lab_creatinine_kinase_flag + lab_bnp_flag + lab_troponin_flag\"\n",
    "    eq2 = smf.mnlogit(f_string2, data=dataframe)\n",
    "    eq2_model = eq2.fit(disp=0)\n",
    "    \n",
    "    # Calculating P(a|c)\n",
    "    f_string3 = \"echo ~ first_careunit + age + gender + weight + saps + sofa + elix_score + vent + \\\n",
    "                vaso + icu_adm_weekday + icu_adm_hour + icd_chf + icd_afib + icd_renal + icd_liver + icd_copd + \\\n",
    "                icd_cad + icd_stroke + icd_malignancy + vs_heart_rate_first + vs_map_first + vs_temp_first + \\\n",
    "                lab_hemoglobin_first + lab_platelet_first + lab_wbc_first + lab_ph_first + lab_chloride_first + \\\n",
    "                lab_sodium_first + lab_bun_first + lab_bicarbonate_first + lab_pco2_first + lab_creatinine_first + \\\n",
    "                lab_potassium_first + lab_po2_first + lab_lactate_first + sedative + vs_cvp_flag + \\\n",
    "                lab_creatinine_kinase_flag + lab_bnp_flag + lab_troponin_flag\"\n",
    "    eq3 = smf.logit(f_string3, data=dataframe)\n",
    "    eq3_model = eq3.fit(disp=0)\n",
    "    \n",
    "    return eq1_model, eq2_model, eq3_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac2713",
   "metadata": {},
   "source": [
    "### Implementing Risk Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3db7275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_ratio(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as three trained models \n",
    "    from generate_models(), calculate the risk ratio as defined by: \n",
    "    causal_effect = summation(c,u){ p(c,u) * ( E[Y=1 | A=1,c,u*] / E[Y=1 | A=0,c,u*] ) }\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 for not \n",
    "           receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(a | c)\n",
    "        4) Default prediction is probability of getting 1 due to how statsmodels works\n",
    "    '''\n",
    "    \n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    # Understanding Matrix of Error Adjustments\n",
    "    confusion = [\n",
    "                    [8, 0, 2, 1],\n",
    "                    [4, 4, 3, 0],\n",
    "                    [1, 0, 14, 1],\n",
    "                    [1, 0, 1, 61]\n",
    "                ] # rows represent the ground truth labels and cols represents the predicted labels\n",
    "\n",
    "    error_mat = [\n",
    "                    [8/11, 0, 2/11, 1/11],\n",
    "                    [4/11, 4/11, 3/11, 0],\n",
    "                    [1/16, 0, 14/16, 1/16],\n",
    "                    [1/63, 0, 1/63, 61/63]\n",
    "                ] # rows represent U* and cols represent U\n",
    "    inverse = np.linalg.pinv(error_mat)\n",
    "    \n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "        \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    comp_0 = upper_0a / lower_0b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    comp_1 = upper_1a / lower_1b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    comp_2 = upper_2a / lower_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    comp_3 = upper_3a / lower_3b\n",
    "    \n",
    "    # Getting P(u | c) \n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "    rr = np.mean(comp_0 * prob_u0_c) + np.mean(comp_1 * prob_u1_c) + np.mean(comp_2 * prob_u2_c) \\\n",
    "            + np.mean(comp_3 * prob_u3_c)\n",
    "    sub_array = [np.mean(comp_0), np.mean(comp_1), np.mean(comp_2), np.mean(comp_3)]\n",
    "    return rr, sub_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae5d0f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8768839707583874,\n",
       " [0.8615914941715498,\n",
       "  0.9221790334361804,\n",
       "  1.1265191373553538,\n",
       "  0.880485625812281])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2, m3 = generate_models(merged_df)\n",
    "risk_ratio(merged_df, m1, m2, m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81961910",
   "metadata": {},
   "source": [
    "### Bootstrapping Merged Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6be00c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bs_rr': 0.8752277806789786,\n",
       " 'bs_arr_rr': [0.8792620765965039,\n",
       "  0.879740100838371,\n",
       "  0.8782733297481601,\n",
       "  0.8428956143727323,\n",
       "  0.8733607375901483,\n",
       "  0.871982494989969,\n",
       "  0.8722203887567788,\n",
       "  0.8888133088559683,\n",
       "  0.8903239853159824,\n",
       "  0.8611063945630258,\n",
       "  0.8773958964255891,\n",
       "  0.8885152502074745,\n",
       "  0.8534523209111112,\n",
       "  0.8654019470719876,\n",
       "  0.8723176785933369,\n",
       "  0.86504731169704,\n",
       "  0.8871564446671825,\n",
       "  0.8839019256220305,\n",
       "  0.8699253451202827,\n",
       "  0.8772146748420085,\n",
       "  0.8645474507844941,\n",
       "  0.885143805924025,\n",
       "  0.8697895485713791,\n",
       "  0.8828761369951353,\n",
       "  0.8730714832253195,\n",
       "  0.8804504075019648,\n",
       "  0.8715413568544357,\n",
       "  0.8583918139276203,\n",
       "  0.8714038787433396,\n",
       "  0.8920070306932663,\n",
       "  0.8718193576318638,\n",
       "  0.8859767367134652,\n",
       "  0.88121141777159,\n",
       "  0.8805594373438067,\n",
       "  0.8684900444718341,\n",
       "  0.8608751470984348,\n",
       "  0.8823092030923647,\n",
       "  0.8801510079093401,\n",
       "  0.873426441179112,\n",
       "  0.8803368931089892,\n",
       "  0.8850021696699721,\n",
       "  0.871188462493574,\n",
       "  0.8766819885398288,\n",
       "  0.8464785991440389,\n",
       "  0.8845832155647315,\n",
       "  0.8790468401384196,\n",
       "  0.8594361668923841,\n",
       "  0.8859199656860808,\n",
       "  0.8836361378486344,\n",
       "  0.8765254058000241,\n",
       "  0.8799180419901061,\n",
       "  0.8615261882947116,\n",
       "  0.8806419008948234,\n",
       "  0.872140744139737,\n",
       "  0.8773933147458901,\n",
       "  0.8785439059328828,\n",
       "  0.8621437042835937,\n",
       "  0.8724451081030657,\n",
       "  0.8480865596137819,\n",
       "  0.8789502342881446,\n",
       "  0.8917472948830083,\n",
       "  0.8700815188671125,\n",
       "  0.8738184236461003,\n",
       "  0.8875763192016423,\n",
       "  0.8866983406863821,\n",
       "  0.8724468684817374,\n",
       "  0.8595544038304026,\n",
       "  0.8607363669445917,\n",
       "  0.8740479504482965,\n",
       "  0.8762566810398994,\n",
       "  0.8738188665599882,\n",
       "  0.8939145469255448,\n",
       "  0.8822279563406252,\n",
       "  0.8930557138452309,\n",
       "  0.8732871161288306,\n",
       "  0.8948848481721923,\n",
       "  0.8589540284775444,\n",
       "  0.8915882423621173,\n",
       "  0.882965694017663,\n",
       "  0.8792612095491436,\n",
       "  0.8851723347357531,\n",
       "  0.8813034201187023,\n",
       "  0.8877296465197632,\n",
       "  0.8788925984116109,\n",
       "  0.8859950447684426,\n",
       "  0.8773003184237,\n",
       "  0.8558170708464505,\n",
       "  0.8707505424408921,\n",
       "  0.8755026744274383,\n",
       "  0.8772822588962272,\n",
       "  0.8589712875150286,\n",
       "  0.866501615789792,\n",
       "  0.8516996238489887,\n",
       "  0.8738110559768506,\n",
       "  0.8808468334217983,\n",
       "  0.8657315023149672,\n",
       "  0.8859474736544607,\n",
       "  0.8782532475395576,\n",
       "  0.8780186088920928,\n",
       "  0.8773540385553942],\n",
       " 'sub_avg_rr': array([0.85965938, 0.92216802, 1.11459588, 0.86276975]),\n",
       " 'sub_arr_rr': array([[0.84131262, 0.92215665, 1.25665265, 0.67666194],\n",
       "        [0.90236096, 0.92096041, 1.47471464, 0.77964926],\n",
       "        [0.81005539, 0.92205148, 0.77380923, 0.88362114],\n",
       "        [0.74185451, 0.92286702, 1.30670347, 0.87829258],\n",
       "        [0.83989157, 0.92153263, 0.79595945, 0.88990682],\n",
       "        [0.86046191, 0.92273247, 1.04685906, 0.95241966],\n",
       "        [0.88570059, 0.92145681, 0.99835682, 0.78857355],\n",
       "        [0.92644421, 0.92287397, 0.865186  , 0.87220096],\n",
       "        [0.93913794, 0.92145485, 1.56903687, 0.88241298],\n",
       "        [0.87704423, 0.92122087, 1.2334152 , 0.93964793],\n",
       "        [0.89685944, 0.92257566, 1.36095756, 0.84131442],\n",
       "        [0.93676305, 0.92244126, 1.19306884, 0.74104363],\n",
       "        [0.90947607, 0.92196426, 1.19702997, 0.91571197],\n",
       "        [0.80593002, 0.92206032, 0.76216003, 0.73153991],\n",
       "        [0.82633478, 0.92261562, 1.21906619, 0.81160613],\n",
       "        [0.79788906, 0.92143084, 0.79928873, 0.8026551 ],\n",
       "        [0.88551665, 0.92190321, 1.89763994, 0.96586061],\n",
       "        [0.90499243, 0.92325833, 1.2163683 , 0.882461  ],\n",
       "        [0.79958633, 0.92220952, 1.48124556, 0.71961606],\n",
       "        [0.79868551, 0.92235662, 1.86610622, 0.87327569],\n",
       "        [0.76633638, 0.92336392, 1.44374783, 0.79856665],\n",
       "        [0.88498655, 0.92086999, 1.14769884, 0.88532611],\n",
       "        [0.75665803, 0.92179352, 0.82454625, 0.81466278],\n",
       "        [0.8476381 , 0.92191033, 0.80410951, 0.91575155],\n",
       "        [0.92089902, 0.9214158 , 0.72390715, 0.8956621 ],\n",
       "        [0.82833838, 0.9211652 , 1.13153654, 0.91025201],\n",
       "        [0.85006185, 0.92001984, 1.09275799, 0.93165099],\n",
       "        [0.79136674, 0.9218915 , 1.68144594, 0.89658977],\n",
       "        [0.87859391, 0.92247535, 1.09952717, 0.87645793],\n",
       "        [0.91951518, 0.92302168, 1.15636378, 0.92557406],\n",
       "        [0.92125369, 0.92224587, 0.97087697, 0.8709092 ],\n",
       "        [0.85974401, 0.92136124, 1.14936202, 1.01291795],\n",
       "        [0.85710866, 0.92183315, 0.85842569, 0.94933309],\n",
       "        [0.86813924, 0.92385472, 0.87199015, 0.73378971],\n",
       "        [0.78895027, 0.92127493, 1.14393904, 0.9372574 ],\n",
       "        [0.84770809, 0.9223562 , 1.26473178, 0.86001799],\n",
       "        [0.84888083, 0.92269968, 0.79594908, 0.72357913],\n",
       "        [0.88714955, 0.92068485, 0.84278715, 0.69796589],\n",
       "        [0.82594616, 0.92199675, 1.50238256, 0.96079976],\n",
       "        [0.8704829 , 0.92221123, 1.37112862, 0.88495118],\n",
       "        [0.91368936, 0.92154499, 1.04794823, 0.90292257],\n",
       "        [0.85229849, 0.92341211, 1.68687062, 0.93054774],\n",
       "        [0.83609251, 0.92321299, 1.27645129, 0.92893736],\n",
       "        [0.85482589, 0.92137332, 0.86241792, 0.90787392],\n",
       "        [0.83646262, 0.9223494 , 1.60109019, 0.94769958],\n",
       "        [0.85135651, 0.92249638, 1.05820288, 0.95002914],\n",
       "        [0.85787633, 0.9201245 , 1.28566372, 0.54945282],\n",
       "        [0.90224335, 0.92226308, 1.45491524, 0.92869183],\n",
       "        [0.90771578, 0.9215134 , 1.03300516, 0.92308777],\n",
       "        [0.86424548, 0.92355572, 0.62679225, 0.97910519],\n",
       "        [0.84372344, 0.92122302, 1.83024117, 0.88510862],\n",
       "        [0.81827853, 0.92219254, 0.60541968, 0.62226543],\n",
       "        [0.85567514, 0.92304073, 0.88021757, 0.67851384],\n",
       "        [0.84704003, 0.92165159, 1.082669  , 0.97097844],\n",
       "        [0.89499747, 0.92148925, 1.40387487, 0.86024475],\n",
       "        [0.81955899, 0.92382795, 1.12746088, 0.96590996],\n",
       "        [0.78605562, 0.92281505, 1.71698947, 0.89639427],\n",
       "        [0.86671387, 0.92075107, 1.34634614, 0.89263454],\n",
       "        [0.82206884, 0.92342919, 1.2873725 , 0.94876767],\n",
       "        [0.91580447, 0.92265993, 0.7522949 , 0.85779321],\n",
       "        [0.87637321, 0.92247025, 0.82466943, 1.00034542],\n",
       "        [0.80912158, 0.92190117, 1.09802037, 0.68942713],\n",
       "        [0.86494231, 0.92231165, 0.85126894, 0.85362926],\n",
       "        [0.90282894, 0.92305291, 1.59340108, 0.92596015],\n",
       "        [0.86933469, 0.92103519, 1.05030054, 0.93459833],\n",
       "        [0.89440073, 0.92202909, 0.81829093, 0.95561058],\n",
       "        [0.90147852, 0.92104833, 1.08643359, 0.95273831],\n",
       "        [0.85804084, 0.92212544, 1.20090248, 0.89382768],\n",
       "        [0.77196207, 0.92216237, 1.19162942, 0.94801595],\n",
       "        [0.79694549, 0.92109634, 1.97089646, 0.88960143],\n",
       "        [0.86443862, 0.92213473, 1.46176926, 0.63624945],\n",
       "        [0.88214156, 0.92238948, 0.9466414 , 0.93521018],\n",
       "        [0.86329292, 0.92193149, 0.82047016, 0.68206827],\n",
       "        [0.89080421, 0.92274985, 1.16357715, 0.6378404 ],\n",
       "        [0.81863092, 0.92195397, 1.19015682, 0.95136658],\n",
       "        [0.87246368, 0.92454371, 0.87501125, 0.93634646],\n",
       "        [0.86019649, 0.92208864, 0.5502487 , 0.92088292],\n",
       "        [0.89499899, 0.92323479, 1.30839308, 0.77779357],\n",
       "        [0.81135089, 0.92155692, 0.83830994, 0.96457902],\n",
       "        [0.8641285 , 0.92392909, 1.15671103, 0.86691127],\n",
       "        [0.86327919, 0.92455405, 0.81806605, 0.78595178],\n",
       "        [0.91255315, 0.92228654, 0.77684064, 0.86133218],\n",
       "        [0.91882825, 0.92158303, 1.10559406, 0.91286069],\n",
       "        [0.88360109, 0.92151629, 1.52560659, 0.89000755],\n",
       "        [0.92000967, 0.92338865, 0.79152235, 0.84882643],\n",
       "        [0.85051351, 0.9226975 , 0.78409801, 0.85155468],\n",
       "        [0.8573478 , 0.92100291, 0.71383233, 0.75785813],\n",
       "        [0.87578735, 0.92149872, 0.81928682, 0.65888954],\n",
       "        [0.90019373, 0.92338729, 1.79790601, 0.82108953],\n",
       "        [0.88512931, 0.92111092, 0.72456228, 0.80446239],\n",
       "        [0.84488589, 0.92353465, 1.03973898, 0.75873719],\n",
       "        [0.85277651, 0.92302879, 1.15574673, 0.93419851],\n",
       "        [0.81332276, 0.92134141, 0.90571089, 0.85486368],\n",
       "        [0.88618973, 0.92334329, 0.69917803, 0.99906448],\n",
       "        [0.88799475, 0.92181661, 0.9150601 , 0.93090422],\n",
       "        [0.8177163 , 0.92137573, 1.34842315, 0.9014148 ],\n",
       "        [0.85939742, 0.92287581, 0.8446826 , 0.84896927],\n",
       "        [0.85322239, 0.92033634, 0.6872975 , 0.88966653],\n",
       "        [0.92402466, 0.92247545, 1.11294171, 0.83228282],\n",
       "        [0.83851227, 0.92240803, 0.73930864, 0.94359323]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap_merged_data_rr(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as three trained models \n",
    "    from generate_models(), perform bootstrapping by shuffling the merged dataframe for\n",
    "    risk ratio calculations. Iterations set to 100.\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 \n",
    "           for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(a | c)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    iterations = 100\n",
    "    output = []\n",
    "    sub_matrix = np.zeros((iterations, 4))\n",
    "    for i in range(iterations):\n",
    "        bt_df = dataframe.sample(frac=1, replace=True, ignore_index=True)\n",
    "        rr, sub_array = risk_ratio(bt_df, model1, model2, model3)\n",
    "        output.append(rr)\n",
    "\n",
    "        for idx, c in enumerate(sub_array):\n",
    "            sub_matrix[i, idx] = c\n",
    "        sub_avg = sub_matrix.mean(axis=0)\n",
    "        \n",
    "    res_dict = {\"bs_rr\": sum(output) / len(output), \"bs_arr_rr\": output, \"sub_avg_rr\": sub_avg, \"sub_arr_rr\": sub_matrix}\n",
    "    \n",
    "    return res_dict\n",
    "\n",
    "bt_merged_rr = bootstrap_merged_data_rr(merged_df, m1, m2, m3)\n",
    "bt_merged_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "464379ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8498027651255051, 0.8925575893480476]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing 95% interval for risk ratio while boostrapping merged dataframe \n",
    "compute_ci_95(bt_merged_rr[\"bs_arr_rr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa45d7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76900858, 0.92050188, 0.65553224, 0.63700515],\n",
       "       [0.92529492, 0.92389376, 1.84907032, 0.98958382]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing 95% interval for risk ratio subgroups while bootstrapping merged dataframe\n",
    "np.apply_along_axis(compute_ci_95, axis=0, arr=bt_merged_rr[\"sub_arr_rr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18fa90",
   "metadata": {},
   "source": [
    "### Bootstrapping Error Rate Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c87a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_ratio_bootstrap(dataframe, model1, model2, model3, error_mat):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe, three trained models \n",
    "    from generate_models(), and an error rate matrix, perform bootstrapping on error-rate matrix for\n",
    "    risk-ratio calculations. Helper function for bootstrap()\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 \n",
    "           for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(a | c)\n",
    "    '''\n",
    "    \n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    # Inversing Error Rate Matrices\n",
    "    inverse = np.linalg.pinv(error_mat)\n",
    "    \n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "        \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    comp_0 = upper_0a / lower_0b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    comp_1 = upper_1a / lower_1b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    comp_2 = upper_2a / lower_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    comp_3 = upper_3a / lower_3b\n",
    "    \n",
    "    # Getting P(u | c) \n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "    rr = np.mean(comp_0 * prob_u0_c) + np.mean(comp_1 * prob_u1_c) + np.mean(comp_2 * prob_u2_c) \\\n",
    "        + np.mean(comp_3 * prob_u3_c)\n",
    "    sub_array = [np.mean(comp_0), np.mean(comp_1), np.mean(comp_2), np.mean(comp_3)]\n",
    "    return rr, sub_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c53e94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a dataframe and 3 models generated from generate_models(), bootstrap the testing set \n",
    "    for n2c2 2006 smoking dataset to get different error rate matrices to test robustness of the \n",
    "    risk ratio casual effect. Iterations set to 100\n",
    "    Utilize predict_bootstrap_2006.py to generate pickle files that store the confusion matrices.\n",
    "    '''\n",
    "    \n",
    "    # Iterating through the bootstrapped confusion matrices \n",
    "    # \"iterations\" var depends on how many bootstrapped confusion matrics were generated\n",
    "    # Default in predict_bootstrap_2006.py is 10\n",
    "    iterations = 100 \n",
    "    rr_arr = []\n",
    "    sub_matrix = np.zeros((iterations, 4))\n",
    "\n",
    "    for x in range(iterations):\n",
    "        # Access each pickle file containing the confusion matrix\n",
    "        f = open(\"...\", \"rb\")  # First input should be the bootstrapped matrices (pkl file)\n",
    "        con_matrix = pickle.load(f)\n",
    "        res = con_matrix/con_matrix.sum(axis=1)[:,None]\n",
    "        \n",
    "        rr, sub_array = risk_ratio_bootstrap(dataframe, model1, model2, model3, res)\n",
    "        rr_arr.append(rr)\n",
    "        \n",
    "        for idx, c in enumerate(sub_array):\n",
    "            sub_matrix[x, idx] = c\n",
    "        sub_avg = sub_matrix.mean(axis=0)\n",
    "        \n",
    "    res_dict = {\"bt_rr\": sum(rr_arr) / len(rr_arr), \"bt_arr_rr\": rr_arr, \"sub_avg_rr\": sub_avg, \"sub_arr_rr\": sub_matrix}\n",
    "    \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ad238fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bt_rr': 0.8995671878792302,\n",
       " 'bt_arr_rr': [0.8950954782552878,\n",
       "  0.9047682189305981,\n",
       "  0.9024360993052649,\n",
       "  0.7820851085032969,\n",
       "  0.9099977515074462,\n",
       "  0.8880588659480342,\n",
       "  0.888529896094769,\n",
       "  0.9286474567985643,\n",
       "  0.7443549274754073,\n",
       "  0.9041411696314656,\n",
       "  0.8684501951365271,\n",
       "  0.8953221311969137,\n",
       "  0.9066492421042313,\n",
       "  0.8936365934404481,\n",
       "  0.8897086112149076,\n",
       "  0.8636844655752582,\n",
       "  0.8929012936783124,\n",
       "  0.9073328098746227,\n",
       "  0.7607746404195312,\n",
       "  0.8876644876491563,\n",
       "  0.8970195567337296,\n",
       "  0.8865806128818529,\n",
       "  0.8600945902403181,\n",
       "  0.8903753123280905,\n",
       "  0.9021971260233119,\n",
       "  0.8793387276042635,\n",
       "  0.8905527526190763,\n",
       "  0.898902067455879,\n",
       "  0.9116582275508983,\n",
       "  0.9156473700721312,\n",
       "  0.917372000533466,\n",
       "  0.8791637555366583,\n",
       "  0.9479073334127955,\n",
       "  0.8802116817135918,\n",
       "  0.9351401332045786,\n",
       "  0.8936502044048605,\n",
       "  0.9509355266273828,\n",
       "  0.9413917371100284,\n",
       "  0.8954072967383542,\n",
       "  0.9105995505469658,\n",
       "  0.8946849369969933,\n",
       "  0.8975567386666485,\n",
       "  0.8765661029104694,\n",
       "  0.8997857241604852,\n",
       "  0.893335096254735,\n",
       "  0.8766918048037393,\n",
       "  0.9067121756640866,\n",
       "  0.8962104895301894,\n",
       "  0.9043309155704095,\n",
       "  0.889503917772057,\n",
       "  0.8929339245939529,\n",
       "  0.8911483664762312,\n",
       "  0.7472462498717095,\n",
       "  0.9017633109559002,\n",
       "  0.9470778213838867,\n",
       "  0.9339177485828531,\n",
       "  0.8892807286126445,\n",
       "  1.3224916642910738,\n",
       "  0.8802047776154157,\n",
       "  0.8934280129698837,\n",
       "  0.8689221465880885,\n",
       "  0.8754633220579138,\n",
       "  0.9151650342059229,\n",
       "  0.9021081778419847,\n",
       "  0.9033188275649765,\n",
       "  0.8914709754060404,\n",
       "  0.9008687872123418,\n",
       "  1.0005720630921586,\n",
       "  0.8889867689827856,\n",
       "  0.8906154254204716,\n",
       "  0.9231425916840591,\n",
       "  0.8782664550811727,\n",
       "  0.8925580612487605,\n",
       "  0.889062840505094,\n",
       "  0.9059076716944181,\n",
       "  0.8942949970972931,\n",
       "  0.9021004975526186,\n",
       "  0.9007497469542505,\n",
       "  0.9503261326598206,\n",
       "  0.8945175464962014,\n",
       "  0.9008304416284059,\n",
       "  0.8947016060484878,\n",
       "  0.9729974234521609,\n",
       "  0.8994899806121207,\n",
       "  0.8997487419905319,\n",
       "  0.8956590840061796,\n",
       "  0.8844672724395441,\n",
       "  0.8760031788210111,\n",
       "  0.9257585425966912,\n",
       "  0.9035032051312526,\n",
       "  0.9043364902775235,\n",
       "  0.8848250332274198,\n",
       "  0.9128679293475063,\n",
       "  0.8693167527157695,\n",
       "  0.8081787891482792,\n",
       "  0.8953689624537217,\n",
       "  0.9344089568750323,\n",
       "  0.832060941472311,\n",
       "  1.072027959054427,\n",
       "  0.8884939135166086],\n",
       " 'sub_avg_rr': array([0.90273646, 0.9218638 , 0.81109834, 0.86229868]),\n",
       " 'sub_arr_rr': array([[ 0.89267825,  0.92217903,  0.87567847,  0.92084167],\n",
       "        [ 0.88959671,  0.92217903,  0.94581198,  0.60092245],\n",
       "        [ 0.92207237,  0.92217903,  0.88643499,  0.90384751],\n",
       "        [ 0.84806232,  0.92217903, -0.09371659,  0.96830583],\n",
       "        [ 0.90036589,  0.92217903,  0.88492904,  0.79508222],\n",
       "        [ 0.89158613,  0.92217903,  0.82327333,  0.90384751],\n",
       "        [-0.25206851,  0.92217903,  0.39458144,  0.91456629],\n",
       "        [ 0.96677783,  0.92217903,  1.00649044,  0.90384751],\n",
       "        [ 0.83924197,  0.92217903,  3.8862616 ,  0.9116723 ],\n",
       "        [ 0.89894099,  0.92217903,  0.85995729,  0.90384751],\n",
       "        [ 0.89148692,  0.92217903,  0.42286122,  0.75663291],\n",
       "        [ 0.89931828,  0.92217903,  0.86085753,  1.01353591],\n",
       "        [ 0.89809422,  0.92217903,  1.05370084,  0.88669143],\n",
       "        [ 0.89036016,  0.92217903,  0.85576959,  0.90384751],\n",
       "        [ 0.84049171,  0.92217903,  0.87228427,  0.97903067],\n",
       "        [ 0.39228267,  0.92217903,  0.84689915,  0.92261482],\n",
       "        [ 1.22385493,  0.92217903, -3.54795995,  0.91101483],\n",
       "        [ 0.96642455,  0.92217903,  1.20424188,  0.85896156],\n",
       "        [ 0.8761247 ,  0.92217903,  0.23157174,  0.90916681],\n",
       "        [ 0.89182004,  0.92217903,  0.86244359,  0.91120014],\n",
       "        [ 0.88204001,  0.92217903,  0.97867532,  0.89084111],\n",
       "        [ 0.90465834,  0.92217903,  0.95664039,  1.05538122],\n",
       "        [ 0.89835866,  0.92217903,  1.06705012,  0.91803399],\n",
       "        [ 0.86917584,  0.92217903,  0.93704621,  1.17103154],\n",
       "        [ 0.8922173 ,  0.92217903,  0.67235011,  0.69947409],\n",
       "        [ 1.24146165,  0.92217903,  0.86356826,  0.84969119],\n",
       "        [ 0.85373954,  0.92217903,  0.90047625,  0.99404735],\n",
       "        [ 0.87055477,  0.92217903,  0.89478077,  0.9783936 ],\n",
       "        [ 0.89663998,  0.92217903,  0.93237312,  0.93778239],\n",
       "        [ 0.97601817,  0.92217903,  1.03612009,  0.90384751],\n",
       "        [ 1.03290992,  0.92217903,  1.08740655,  1.04507073],\n",
       "        [ 0.86533551,  0.92217903,  0.8246632 ,  0.90384751],\n",
       "        [ 0.98749152,  0.92217903,  1.00184985,  0.89045864],\n",
       "        [ 0.89187771,  0.92217903,  1.10760677,  1.34539819],\n",
       "        [ 1.09202338,  0.92217903,  0.87233138,  0.86296592],\n",
       "        [ 0.94271219,  0.92217903,  0.88800543,  0.89545036],\n",
       "        [ 0.90028861,  0.92217903,  1.32988779,  1.32535454],\n",
       "        [ 0.88152977,  0.92217903,  1.2610493 ,  0.86265316],\n",
       "        [ 0.91333703,  0.92217903,  0.9197198 ,  0.90384751],\n",
       "        [ 0.89290538,  0.92217903,  0.97941703,  0.93997569],\n",
       "        [ 0.86517546,  0.92217903,  0.85333834,  0.93226498],\n",
       "        [ 0.88600917,  0.92217903,  0.86635728,  0.91073061],\n",
       "        [ 0.69864465,  0.92217903,  0.8747731 ,  0.91212465],\n",
       "        [ 0.89647674,  0.92217903,  0.87812484,  0.88604985],\n",
       "        [ 0.89150136,  0.92217903,  0.91525458,  0.9149701 ],\n",
       "        [ 0.24752737,  0.92217903,  0.44055036,  1.14330247],\n",
       "        [ 0.85614973,  0.92217903,  0.59192448,  0.91151715],\n",
       "        [ 0.96210456,  0.92217903,  0.87232307,  0.61690758],\n",
       "        [ 0.89197795,  0.92217903,  0.83310341,  1.04760057],\n",
       "        [ 0.76925188,  0.92217903,  0.85984207,  0.94174844],\n",
       "        [ 0.920299  ,  0.92217903,  0.86545328,  0.91361669],\n",
       "        [ 0.89835866,  0.92217903,  0.68933642,  0.90591582],\n",
       "        [ 0.56253806,  0.92217903,  3.35655118,  0.93877049],\n",
       "        [ 0.9017557 ,  0.92217903,  0.71633738,  1.00104558],\n",
       "        [ 0.90134955,  0.92217903,  1.0519144 ,  0.91167011],\n",
       "        [-0.38436731,  0.92217903, -7.52333224,  0.84978062],\n",
       "        [ 0.79940303,  0.92217903,  0.86085753,  0.9702782 ],\n",
       "        [ 3.04185198,  0.92217903,  0.96507337,  0.94765126],\n",
       "        [ 1.62942632,  0.92217903,  0.61520623,  0.90384751],\n",
       "        [ 0.90525674,  0.92217903,  0.90043743,  0.90384751],\n",
       "        [ 0.76459845,  0.92217903,  0.78085968,  0.90384751],\n",
       "        [ 0.88933051,  0.92217903,  0.83243447,  0.90352707],\n",
       "        [ 0.89598779,  0.92217903,  1.01062605,  0.91273513],\n",
       "        [ 0.90591938,  0.92217903,  0.83911295,  0.91443354],\n",
       "        [ 0.89086707,  0.92217903,  0.77171884,  0.98446596],\n",
       "        [ 0.90479184,  0.92217903,  0.86946343,  0.91143659],\n",
       "        [ 0.9013336 ,  0.92217903,  0.65869893,  0.9325192 ],\n",
       "        [ 1.38681271,  0.92217903,  0.883809  ,  0.92117693],\n",
       "        [ 0.89152078,  0.92217903,  0.87564049, -2.85938202],\n",
       "        [ 0.89948463,  0.92217903,  0.79948798,  1.12585722],\n",
       "        [ 0.89233421,  0.92217903,  0.82011985,  0.48297558],\n",
       "        [ 0.90203271,  0.92217903,  0.34096628,  0.91903064],\n",
       "        [ 0.90728539,  0.92217903,  0.83586653,  1.12937766],\n",
       "        [ 0.90044112,  0.92217903,  0.8705607 ,  0.90384751],\n",
       "        [ 0.89213594,  0.92217903,  0.8971795 ,  0.90384751],\n",
       "        [ 0.88157566,  0.92217903,  0.88148268,  0.90384751],\n",
       "        [ 0.90331259,  0.92217903,  0.90274583,  0.91330902],\n",
       "        [ 0.8696751 ,  0.92217903,  0.97571103,  0.91858914],\n",
       "        [ 1.06979583,  0.92217903,  0.81272014,  0.93500107],\n",
       "        [ 0.89259777,  0.92217903,  0.87567847,  0.91769038],\n",
       "        [ 0.9050854 ,  0.92217903,  0.89315566,  0.91123475],\n",
       "        [ 0.90216389,  0.92217903,  0.80331862,  0.88503564],\n",
       "        [ 0.83512388,  0.92217903,  0.90060014, -1.19794718],\n",
       "        [ 0.89130334,  0.92217903,  1.03491327,  0.90384751],\n",
       "        [ 0.89051249,  0.92217903,  0.82620018,  0.89992105],\n",
       "        [ 0.88103368,  0.92217903,  0.90551473,  0.90878831],\n",
       "        [ 0.81160266,  0.92217903,  0.96060394,  0.93550809],\n",
       "        [ 0.89995469,  0.92217903,  0.7426348 ,  0.90996796],\n",
       "        [ 1.01199557,  0.92217903,  0.90701109,  0.92079617],\n",
       "        [ 0.90112765,  0.92217903,  0.91802826,  0.93829728],\n",
       "        [ 0.95498989,  0.92217903,  0.94494993,  0.91364879],\n",
       "        [ 0.92198724,  0.92217903,  0.81998979,  0.8133789 ],\n",
       "        [ 0.85116737,  0.92217903,  1.02180277,  0.90073809],\n",
       "        [ 0.93031333,  0.92217903,  0.8080093 ,  0.92665895],\n",
       "        [ 1.35300065,  0.92217903,  0.92024422,  0.91965665],\n",
       "        [ 0.90795741,  0.92217903,  0.94535351,  0.90384751],\n",
       "        [ 0.90081073,  0.92217903,  0.86811459,  0.90384751],\n",
       "        [ 0.91731479,  0.890656  ,  0.56195561,  0.91323624],\n",
       "        [ 0.8763838 ,  0.92217903,  3.71502373,  0.90384751],\n",
       "        [ 0.89450649,  0.92217903,  0.95071065,  0.92076142]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2, m3 = generate_models(merged_df)\n",
    "bt_rr = bootstrap(merged_df, m1, m2, m3)\n",
    "bt_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73c61af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7708971127593199, 0.9874741092631595]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing 95% interval for risk ratio while boostrapping error rate matrix\n",
    "compute_ci_95(bt_rr[\"bt_arr_rr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88c9d813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31628614, 0.92217903, 0.06079537, 0.53900034],\n",
       "       [1.37075198, 0.92217903, 2.39388607, 1.15786023]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing 95% interval for risk ratio subgroups while\n",
    "# bootstrapping error rate matrix\n",
    "np.apply_along_axis(compute_ci_95, axis=0, arr=bt_rr[\"sub_arr_rr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9cd103",
   "metadata": {},
   "source": [
    "### Combined bootstrapping for RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "578dc786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_bootstrap_rr(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a dataframe and 3 models generated from generate_models(), do combined bootstrapping to see\n",
    "    robustness of risk ratio calculations. Iterations set to 100\n",
    "    Utilize predict_bootstrap_2006.py to generate pickle files that store the confusion matrices.\n",
    "    '''\n",
    "    \n",
    "    # Iterate through 10 of bootstrapped error matrices\n",
    "    iterations_em = 10\n",
    "    rr_arr = []\n",
    "    for iem in range(iterations_em):\n",
    "        f = open(\"...\", \"rb\")\n",
    "        con_matrix = pickle.load(f)\n",
    "        res = con_matrix/con_matrix.sum(axis=1)[:,None]\n",
    "        \n",
    "        # Iterate through 10 bootstrapped (shuffled) dataframes\n",
    "        iterations_df = 10\n",
    "        for idf in range(iterations_em):\n",
    "            bt_df = dataframe.sample(frac=1, replace=True, ignore_index=True)\n",
    "            rr, sub_array = risk_ratio_bootstrap(bt_df, model1, model2, model3, res)\n",
    "            rr_arr.append(rr)\n",
    "    print(\"Number of calcs:\", len(rr_arr)) # == 100 based on default settings\n",
    "    print(\"Mean combined bootstrap rr:\", sum(rr_arr) / len(rr_arr))\n",
    "    return [sum(rr_arr) / len(rr_arr), rr_arr]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5441d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of calcs: 100\n",
      "Mean combined bootstrap rr: 0.8735481793257273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6752182228746604, 0.9440202895674686]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Compute 95% invertal for RR while doing combined bootstrapping\n",
    "combined_rr = combined_bootstrap_rr(merged_df, m1, m2, m3)\n",
    "compute_ci_95(combined_rr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6cbf51",
   "metadata": {},
   "source": [
    "### Implementing OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8025cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_ratio(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as three trained models \n",
    "    from generate_models(), calculate the odds ratio as defined by: \n",
    "    causal_effect = (P(Y^{a=1}=1) * P(Y^{a=0}=0)) / (P(Y^{a=1}=0) * P(Y^{a=0}=1))\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 \n",
    "           for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model5 = P(a | c)\n",
    "        4) Default prediction is probability of getting 1 due to how statsmodels works\n",
    "    '''\n",
    "    \n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    # Creating Matrix of Error Adjustments\n",
    "    confusion = [\n",
    "                    [8, 0, 2, 1],\n",
    "                    [4, 4, 3, 0],\n",
    "                    [1, 0, 14, 1],\n",
    "                    [1, 0, 1, 61]\n",
    "                ] # rows represent the ground truth labels and cols represents the predicted labels\n",
    "\n",
    "    error_mat = [\n",
    "                    [8/11, 0, 2/11, 1/11],\n",
    "                    [4/11, 4/11, 3/11, 0],\n",
    "                    [1/16, 0, 14/16, 1/16],\n",
    "                    [1/63, 0, 1/63, 61/63]\n",
    "                ] # rows represent U* and cols represent U\n",
    "    inverse = np.linalg.pinv(error_mat)\n",
    "    \n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    # Getting P(u | c) \n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "    numerator_a = np.sum(upper_0a * prob_u0_c) + np.sum(upper_1a * prob_u1_c) + \\\n",
    "                  np.sum(upper_2a * prob_u2_c) + np.sum(upper_3a * prob_u3_c)\n",
    "    numerator_b = np.sum((1 - lower_0b) * prob_u0_c) + np.sum((1 - lower_1b) * prob_u1_c) + \\\n",
    "                  np.sum((1 - lower_2b) * prob_u2_c) + np.sum((1 - lower_3b) * prob_u3_c)\n",
    "     \n",
    "    denominator_a = np.sum((1 - upper_0a) * prob_u0_c) + np.sum((1 - upper_1a) * prob_u1_c) + \\\n",
    "                    np.sum((1 - upper_2a) * prob_u2_c) + np.sum((1 - upper_3a) * prob_u3_c)\n",
    "    denominator_b = np.sum(lower_0b * prob_u0_c) + np.sum(lower_1b * prob_u1_c) + \\\n",
    "                    np.sum(lower_2b * prob_u2_c) + np.sum(lower_3b * prob_u3_c)\n",
    "    \n",
    "    numerator = numerator_a * numerator_b\n",
    "    denominator = denominator_a * denominator_b\n",
    "    \n",
    "    return numerator / denominator\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca656b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8864763609817276"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2, m3 = generate_models(merged_df)\n",
    "odds_ratio(merged_df, m1, m2, m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b622b4f",
   "metadata": {},
   "source": [
    "### Bootstrapping merged dataframe for OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8f9a01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8851573696534452"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap_merged_data_or(dataframe, m1, m2, m3):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as three trained models \n",
    "    from generate_models(), perform bootstrapping by shuffling the merged dataframe for\n",
    "    odds ratio calculations. Iterations set to 100.\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 \n",
    "           for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(a | c)\n",
    "    '''\n",
    "    iterations = 100\n",
    "    output = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        bt_data = dataframe.sample(frac=1, replace=True, ignore_index=True)\n",
    "        or_val = odds_ratio(bt_data, m1, m2, m3)\n",
    "        output.append(or_val)\n",
    "\n",
    "    return [sum(output) / len(output), output]\n",
    "\n",
    "bt_merged_or = bootstrap_merged_data_or(merged_df, m1, m2, m3)\n",
    "bt_merged_or[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8520d368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8665715709298584, 0.8999256163715614]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute 95% CI for OR while bootstrapping merged dataframe\n",
    "compute_ci_95(bt_merged_or[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56844e59",
   "metadata": {},
   "source": [
    "### Bootstrapping Error Matrix for OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dafa3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_ratio_bootstrap(dataframe, model1, model2, model3, error_mat):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe, three trained models \n",
    "    from generate_models(), and an error rate matrix, perform bootstrapping on error-rate matrix for\n",
    "    odds ratio calculations. Helper function for bootstrap_or()\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 \n",
    "           for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(a | c)\n",
    "    '''\n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    inverse = np.linalg.pinv(error_mat)\n",
    "    \n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    # Getting P(u | c) \n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "    numerator_a = np.sum(upper_0a * prob_u0_c) + np.sum(upper_1a * prob_u1_c) + \\\n",
    "                  np.sum(upper_2a * prob_u2_c) + np.sum(upper_3a * prob_u3_c)\n",
    "    numerator_b = np.sum((1 - lower_0b) * prob_u0_c) + np.sum((1 - lower_1b) * prob_u1_c) + \\\n",
    "                  np.sum((1 - lower_2b) * prob_u2_c) + np.sum((1 - lower_3b) * prob_u3_c)\n",
    "     \n",
    "    denominator_a = np.sum((1 - upper_0a) * prob_u0_c) + np.sum((1 - upper_1a) * prob_u1_c) + \\\n",
    "                    np.sum((1 - upper_2a) * prob_u2_c) + np.sum((1 - upper_3a) * prob_u3_c)\n",
    "    denominator_b = np.sum(lower_0b * prob_u0_c) + np.sum(lower_1b * prob_u1_c) + \\\n",
    "                    np.sum(lower_2b * prob_u2_c) + np.sum(lower_3b * prob_u3_c)\n",
    "    \n",
    "    numerator = numerator_a * numerator_b\n",
    "    denominator = denominator_a * denominator_b\n",
    "    \n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05c1a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_or(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a dataframe and 5 models generated from generate_models(), bootstrap the testing set \n",
    "    for n2c2 2006 smoking dataset to get different error rate matrices to test robustness of \n",
    "    the odds ratio casual effect.\n",
    "    Utilize predict_bootstrap_2006.py to generate pickle files that store the confusion matrices.\n",
    "    '''\n",
    "    \n",
    "    # Iterating through the bootstrapped confusion matrices \n",
    "    # \"iterations\" var depends on how many bootstrapped confusion matrics were generated\n",
    "    # Default in predict_bootstrap_2006.py is 10\n",
    "    iterations = 100 \n",
    "    o_r_arr = []\n",
    "    for x in range(iterations):\n",
    "        # Access each pickle file containing the confusion matrix\n",
    "        f = open(\"...\", \"rb\")  # First input should be the bootstrapped matrices (pkl file)\n",
    "        con_matrix = pickle.load(f)\n",
    "        res = con_matrix/con_matrix.sum(axis=1)[:,None]\n",
    "        o_r = odds_ratio_bootstrap(dataframe, model1, model2, model3, res)\n",
    "        o_r_arr.append(o_r)\n",
    "    \n",
    "    return [sum(o_r_arr) / len(o_r_arr), o_r_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e87266c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9196165710847475"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2, m3 = generate_models(merged_df)\n",
    "bt_or = bootstrap_or(merged_df, m1, m2, m3)\n",
    "bt_or[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c64d8ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7734094843134075, 0.9803855037359114]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing 95% interval for odds ratio while boostrapping error rate matrix\n",
    "compute_ci_95(bt_or[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37675289",
   "metadata": {},
   "source": [
    "### Combined Bootstrapping for OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "924b5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_bootstrap_or(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a dataframe and 3 models generated from generate_models(), do combined bootstrapping to see\n",
    "    robustness of odds ratio calculations. Iterations set to 100\n",
    "    Utilize predict_bootstrap_2006.py to generate pickle files that store the confusion matrices.\n",
    "    '''\n",
    "    \n",
    "    # Iterate through 10 of bootstrapped error matrices\n",
    "    iterations_em = 10\n",
    "    or_arr = []\n",
    "    for iem in range(iterations_em):\n",
    "        f = open(\"...\", \"rb\")\n",
    "        con_matrix = pickle.load(f)\n",
    "        res = con_matrix/con_matrix.sum(axis=1)[:,None]\n",
    "        \n",
    "        # Iterate through 10 bootstrapped (shuffled) dataframes\n",
    "        iterations_df = 10\n",
    "        for idf in range(iterations_df):\n",
    "            bt_df = dataframe.sample(frac=1, replace=True, ignore_index=True)\n",
    "            or_v = odds_ratio_bootstrap(bt_df, model1, model2, model3, res)\n",
    "            or_arr.append(or_v)\n",
    "    print(len(or_arr)) # == 100 based on default settings\n",
    "    print(\"Mean combined bootstrap or_v:\", sum(or_arr) / len(or_arr))\n",
    "    return [sum(or_arr) / len(or_arr), or_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a71dfca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Mean combined bootstrap or_v: 0.883528044221483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5818542201286497, 0.955810700152887]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Compute 95% invertal for RR while doing combined bootstrapping\n",
    "combined_or = combined_bootstrap_or(merged_df, m1, m2, m3)\n",
    "compute_ci_95(combined_or[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe89e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
